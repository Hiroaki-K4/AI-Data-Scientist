{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06ad243",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Predicting Titanic Survival\n",
    "\n",
    "This code demonstrates a basic machine learning workflow to predict whether a passenger survived the Titanic shipwreck using the `train.csv` dataset.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we'll import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bb2978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T22:19:16.221947Z",
     "iopub.status.busy": "2025-05-14T22:19:16.220161Z",
     "iopub.status.idle": "2025-05-14T22:19:17.084865Z",
     "shell.execute_reply": "2025-05-14T22:19:17.084151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "\n",
      "--- Data Head ---\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "\n",
      "--- Missing Values ---\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('data/titanic/train.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/titanic/train.csv' not found.\")\n",
    "    print(\"Please make sure the train.csv file is in the 'data/titanic/' directory relative to where you run this script.\")\n",
    "    exit() # Exit the script if the file isn't found\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\n--- Data Head ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Display data info and check for missing values\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45362c6",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "The data needs cleaning and transformation before being used by a machine learning model.\n",
    "\n",
    "*   **Handle Missing Values:**\n",
    "    *   `Age`: Impute with the median age.\n",
    "    *   `Fare`: Impute with the median fare (though training data has no missing Fare, good practice).\n",
    "    *   `Embarked`: Impute with the most frequent embarkation port.\n",
    "    *   `Cabin`: Too many missing values, we will drop this column.\n",
    "*   **Handle Categorical Features:**\n",
    "    *   `Sex` and `Embarked`, `Pclass`: Convert these categorical features into numerical representations using one-hot encoding.\n",
    "*   **Feature Selection:**\n",
    "    *   Drop irrelevant columns like `PassengerId`, `Name`, `Ticket`, and `Cabin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8606e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T22:19:17.087127Z",
     "iopub.status.busy": "2025-05-14T22:19:17.086589Z",
     "iopub.status.idle": "2025-05-14T22:19:17.099501Z",
     "shell.execute_reply": "2025-05-14T22:19:17.098826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing defined ---\n",
      "ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median'))]),\n",
      "                                 ['Age', 'SibSp', 'Parch', 'Fare']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['Sex', 'Embarked', 'Pclass'])])\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Select features to use (excluding those to be dropped)\n",
    "# 'PassengerId', 'Name', 'Ticket', 'Cabin' will be dropped implicitly by ColumnTransformer's remainder='drop'\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = X[features] # Keep only the selected feature columns in X\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "# Note: Pclass is technically ordinal but often treated as categorical or numerical.\n",
    "# We'll treat it as categorical here and one-hot encode it.\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "# Numerical pipeline: Impute missing values with median\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute missing values with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # For Embarked\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is useful for prediction on unseen data\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "# 'remainder='drop'' will drop all columns not specified in transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Preprocessing defined ---\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ece21",
   "metadata": {},
   "source": [
    "## 3. Split Data\n",
    "\n",
    "Split the data into training and testing sets. The model will be trained on the training data and evaluated on the unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b122eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T22:19:17.101539Z",
     "iopub.status.busy": "2025-05-14T22:19:17.101235Z",
     "iopub.status.idle": "2025-05-14T22:19:17.106693Z",
     "shell.execute_reply": "2025-05-14T22:19:17.106072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (712, 7)\n",
      "Testing set shape: (179, 7)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd35f2",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "We will train a few different classification models and evaluate their performance using accuracy, classification report (precision, recall, F1-score), and confusion matrix.\n",
    "\n",
    "We use a `Pipeline` to chain the preprocessing steps and the model training together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0fb819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T22:19:17.108842Z",
     "iopub.status.busy": "2025-05-14T22:19:17.108292Z",
     "iopub.status.idle": "2025-05-14T22:19:17.344818Z",
     "shell.execute_reply": "2025-05-14T22:19:17.343990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression ---\n",
      "Logistic Regression Accuracy: 0.7989\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83       105\n",
      "           1       0.78      0.72      0.75        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[90 15]\n",
      " [21 53]]\n",
      "\n",
      "--- Training Decision Tree ---\n",
      "Decision Tree Accuracy: 0.7933\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82       105\n",
      "           1       0.73      0.78      0.76        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.80      0.79      0.79       179\n",
      "\n",
      "Decision Tree Confusion Matrix:\n",
      "[[84 21]\n",
      " [16 58]]\n",
      "\n",
      "--- Training Random Forest ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8212\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       105\n",
      "           1       0.79      0.77      0.78        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      "[[90 15]\n",
      " [17 57]]\n"
     ]
    }
   ],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42), # liblinear is good for small datasets\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "\n",
    "    # Create a pipeline that first preprocesses then trains the model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} Classification Report:\\n{report}\")\n",
    "    print(f\"{name} Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec8328",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1.  Loads the Titanic training data.\n",
    "2.  Inspects the data, including checking for missing values.\n",
    "3.  Defines preprocessing steps using `ColumnTransformer` and `Pipeline` to handle missing values (imputation) and categorical features (one-hot encoding), while dropping irrelevant columns.\n",
    "4.  Splits the data into training and testing sets.\n",
    "5.  Trains and evaluates three different classification models (Logistic Regression, Decision Tree, Random Forest) using a pipeline that includes the preprocessing.\n",
    "6.  Prints the accuracy, classification report, and confusion matrix for each model on the test set.\n",
    "\n",
    "The output shows the performance of each model. You can compare the metrics (accuracy, precision, recall, F1-score) to see which model performed best on this particular split of the data. Random Forest often performs well on this dataset due to its ensemble nature.\n",
    "\n",
    "This is a basic example. Further improvements could include:\n",
    "*   More sophisticated feature engineering (e.g., creating 'FamilySize' from 'SibSp' and 'Parch', extracting titles from 'Name').\n",
    "*   Hyperparameter tuning for the models.\n",
    "*   Cross-validation for more robust evaluation.\n",
    "*   Handling outliers.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
