{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06ad243",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Predicting Titanic Survival\n",
    "\n",
    "This code demonstrates a basic machine learning workflow to predict whether a passenger survived the Titanic shipwreck using the `train.csv` dataset.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we'll import the necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('data/titanic/train.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/titanic/train.csv' not found.\")\n",
    "    print(\"Please make sure the train.csv file is in the 'data/titanic/' directory relative to where you run this script.\")\n",
    "    exit() # Exit the script if the file isn't found\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\n--- Data Head ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Display data info and check for missing values\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45362c6",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "The data needs cleaning and transformation before being used by a machine learning model.\n",
    "\n",
    "*   **Handle Missing Values:**\n",
    "    *   `Age`: Impute with the median age.\n",
    "    *   `Fare`: Impute with the median fare (though training data has no missing Fare, good practice).\n",
    "    *   `Embarked`: Impute with the most frequent embarkation port.\n",
    "    *   `Cabin`: Too many missing values, we will drop this column.\n",
    "*   **Handle Categorical Features:**\n",
    "    *   `Sex` and `Embarked`, `Pclass`: Convert these categorical features into numerical representations using one-hot encoding.\n",
    "*   **Feature Selection:**\n",
    "    *   Drop irrelevant columns like `PassengerId`, `Name`, `Ticket`, and `Cabin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8606e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Select features to use (excluding those to be dropped)\n",
    "# 'PassengerId', 'Name', 'Ticket', 'Cabin' will be dropped implicitly by ColumnTransformer's remainder='drop'\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = X[features] # Keep only the selected feature columns in X\n",
    "\n",
    "# Identify column types for preprocessing\n",
    "# Note: Pclass is technically ordinal but often treated as categorical or numerical.\n",
    "# We'll treat it as categorical here and one-hot encode it.\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "# Numerical pipeline: Impute missing values with median\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute missing values with most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # For Embarked\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is useful for prediction on unseen data\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "# 'remainder='drop'' will drop all columns not specified in transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Preprocessing defined ---\")\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ece21",
   "metadata": {},
   "source": [
    "## 3. Split Data\n",
    "\n",
    "Split the data into training and testing sets. The model will be trained on the training data and evaluated on the unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b122eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd35f2",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "We will train a few different classification models and evaluate their performance using accuracy, classification report (precision, recall, F1-score), and confusion matrix.\n",
    "\n",
    "We use a `Pipeline` to chain the preprocessing steps and the model training together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42), # liblinear is good for small datasets\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "\n",
    "    # Create a pipeline that first preprocesses then trains the model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} Classification Report:\\n{report}\")\n",
    "    print(f\"{name} Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec8328",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The code performs the following steps:\n",
    "\n",
    "1.  Loads the Titanic training data.\n",
    "2.  Inspects the data, including checking for missing values.\n",
    "3.  Defines preprocessing steps using `ColumnTransformer` and `Pipeline` to handle missing values (imputation) and categorical features (one-hot encoding), while dropping irrelevant columns.\n",
    "4.  Splits the data into training and testing sets.\n",
    "5.  Trains and evaluates three different classification models (Logistic Regression, Decision Tree, Random Forest) using a pipeline that includes the preprocessing.\n",
    "6.  Prints the accuracy, classification report, and confusion matrix for each model on the test set.\n",
    "\n",
    "The output shows the performance of each model. You can compare the metrics (accuracy, precision, recall, F1-score) to see which model performed best on this particular split of the data. Random Forest often performs well on this dataset due to its ensemble nature.\n",
    "\n",
    "This is a basic example. Further improvements could include:\n",
    "*   More sophisticated feature engineering (e.g., creating 'FamilySize' from 'SibSp' and 'Parch', extracting titles from 'Name').\n",
    "*   Hyperparameter tuning for the models.\n",
    "*   Cross-validation for more robust evaluation.\n",
    "*   Handling outliers.\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
